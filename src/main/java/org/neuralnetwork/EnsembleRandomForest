package org.ensembles;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.mllib.regression.LabeledPoint;
import org.apache.spark.mllib.tree.RandomForest;
import org.apache.spark.mllib.tree.model.RandomForestModel;
import org.apache.spark.mllib.util.MLUtils;
import scala.Tuple2;

import java.io.Serializable;
import java.text.DecimalFormat;
import java.util.HashMap;

/**
 * Ensemble Random Forest for reason prediction for fraud.
 */
public class RandomForestTraining implements Serializable{

    private HashMap<Integer,Integer> categoricalFeaturesInfo = new HashMap<Integer, Integer>();
    private Integer numTrees = 12;
    private Integer maxDepth = 4;
    private static JavaSparkContext context = null;
    private RandomForestModel model = null;
    private JavaRDD<LabeledPoint> testData = null;
    private static RandomForestTraining forestModel = null;

    private Log log = LogFactory.getLog(RandomForestTraining.class);

    private RandomForestTraining(){

        SparkConf configuration = new SparkConf().setAppName("RandomForestTrain").setMaster("local[*]");
        context = new JavaSparkContext(configuration);
    }

    private RandomForestTraining(int numTrees,int depthOfTree){

        SparkConf configuration = new SparkConf().setAppName("RandomForestTrain").setMaster("local[*]");
        context = new JavaSparkContext(configuration);
        this.numTrees = numTrees;
        this.maxDepth = depthOfTree;
    }

    public void buildModel(String file,double trainingPercentage,double predictPercentage,int classes) throws Exception{

        log.info("Model loading...");
        try {
            JavaRDD<LabeledPoint> data = MLUtils.loadLibSVMFile(context.sc(), file).toJavaRDD();
            JavaRDD<LabeledPoint>[] splitData = data.randomSplit(new double[]{trainingPercentage, predictPercentage});
            JavaRDD<LabeledPoint> trainingData = splitData[0];
            testData = splitData[1];
            String featureSubsetCategory = "auto";
            String impurity = "gini";
            Integer maxBins = 32;
            Integer seed = 12345;
            int numClasses = classes;
            model = RandomForest.trainClassifier(trainingData,numClasses,categoricalFeaturesInfo, numTrees, featureSubsetCategory, impurity, maxDepth, maxBins, seed);
        }catch (Exception e){

            log.error("Error occurred:"+e.getLocalizedMessage());
            throw e;
        }
    }

    public String evaluateModel() throws Exception{

        try {
            JavaPairRDD<Double, Double> testPrediction = testData.mapToPair(new PairFunction<LabeledPoint, Double, Double>() {
                @Override
                public Tuple2<Double, Double> call(LabeledPoint labeledPoint) throws Exception {

                    return new Tuple2<Double, Double>(model.predict(labeledPoint.features()), labeledPoint.label());
                }
            });

            /*Double mse = testPrediction.map(new Function<Tuple2<Double, Double>, Double>() {

                @Override
                public  Double call(Tuple2<Double, Double> prediction) throws Exception {

                    Double difference = prediction._1() - prediction._2();
                    return difference * difference;
                }
            }).reduce(new Function2<Double, Double, Double>() {

                @Override
                public Double call(Double a, Double b) throws Exception {
                    return a + b;
                }
            }) / testData.count();*/
            Double testErr =
                    1.0 * testPrediction.filter(new Function<Tuple2<Double, Double>, Boolean>() {
                        @Override
                        public Boolean call(Tuple2<Double, Double> pl) {
                            return !pl._1().equals(pl._2());
                        }
                    }).count() / testData.count();

            double accuracy = 1 - testErr;
            DecimalFormat df = new DecimalFormat("#.##");
            accuracy = Double.valueOf(df.format(accuracy));
            String output =  "Test Error:" + testErr+"\n"+"Accuracy:"+accuracy*100+"%";
            return output;
        }catch (Exception e){

            log.error("Error occurred:"+e.getLocalizedMessage());
            throw e;
        }

    }

    public void loadSaveModel(String savePath,boolean save){

        if(!save) {

            model = RandomForestModel.load(context.sc(), savePath);
        }else{

            model.save(context.sc(),savePath);
        }
    }

    public static RandomForestTraining getModel(){

        if(forestModel == null){

            forestModel = new RandomForestTraining();
        }
        return forestModel;
    }

    public static RandomForestTraining getModel(int numTrees,int depthOfTree){

        if(forestModel == null){

            forestModel = new RandomForestTraining(numTrees,depthOfTree);
        }
        return forestModel;
    }

    public static void main(String[] args) {

        RandomForestTraining training = RandomForestTraining.getModel();
        try {

            training.buildModel("/home/asantha/javaspringmvc-master/target/assignment-1.0.0-BUILD-SNAPSHOT/datasets/original/data_sample.txt",0.7,0.3,2);
            String output = training.evaluateModel();
            training.loadSaveModel("/home/asantha/javaspringmvc-master/target/assignment-1.0.0-BUILD-SNAPSHOT/datasets/ensembles/RandomForestModel",true);
            System.out.println(output);
        } catch (Exception e) {
            e.printStackTrace();
        }

    }

}
